{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e975cb0d-ccfa-49a4-931d-d7f496bb88b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQLite to BigQuery export tool initialized with export directory: bigquery_exports\n",
      "Database file 'racing_data.db' found.\n",
      "Tables in database: races, runners, sqlite_sequence, sqlite_stat1\n",
      "Found 4 tables: races, runners, sqlite_sequence, sqlite_stat1\n",
      "Exporting table 'races' with 262679 rows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e94f803ce8464bceb088b44a140e0b59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Exporting races:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported races to 1 CSV file(s)\n",
      "Completed export of 'races' in 5.44 seconds\n",
      "Exporting table 'runners' with 2543909 rows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9021865fa264327853aef809620e51b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Exporting runners:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported runners to 6 CSV file(s)\n",
      "Completed export of 'runners' in 50.33 seconds\n",
      "Exporting table 'sqlite_sequence' with 1 rows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a0ba7e72a714364b36b04d29a5d2f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Exporting sqlite_sequence:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported sqlite_sequence to 1 CSV file(s)\n",
      "Completed export of 'sqlite_sequence' in 0.02 seconds\n",
      "Exporting table 'sqlite_stat1' with 6 rows\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e791cc7cd8342248f833f91418498ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Exporting sqlite_stat1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported sqlite_stat1 to 1 CSV file(s)\n",
      "Completed export of 'sqlite_stat1' in 0.02 seconds\n",
      "Export completed in 55.82 seconds\n",
      "Files in export directory (14 files):\n",
      "1. export_summary.json (0.00 MB)\n",
      "2. races.csv (75.12 MB)\n",
      "3. races_schema.json (0.00 MB)\n",
      "4. runners_part1of6.csv (188.29 MB)\n",
      "5. runners_part2of6.csv (192.17 MB)\n",
      "6. runners_part3of6.csv (199.84 MB)\n",
      "7. runners_part4of6.csv (203.08 MB)\n",
      "8. runners_part5of6.csv (204.74 MB)\n",
      "9. runners_part6of6.csv (17.59 MB)\n",
      "10. runners_schema.json (0.00 MB)\n",
      "11. sqlite_sequence.csv (0.00 MB)\n",
      "12. sqlite_sequence_schema.json (0.00 MB)\n",
      "13. sqlite_stat1.csv (0.00 MB)\n",
      "14. sqlite_stat1_schema.json (0.00 MB)\n"
     ]
    }
   ],
   "source": [
    "# SQLite to BigQuery Export\n",
    "# ========================\n",
    "#\n",
    "# This notebook converts a SQLite database (racing_data.db) to formats compatible with\n",
    "# Google BigQuery, specifically CSV or JSON for direct upload.\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime\n",
    "import gc  # For garbage collection\n",
    "import time\n",
    "\n",
    "# Set up directory for exports\n",
    "EXPORT_DIR = \"bigquery_exports\"\n",
    "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
    "\n",
    "# Database connection parameters\n",
    "DB_PATH = 'racing_data.db'  # Update this if your database file is located elsewhere\n",
    "\n",
    "# Maximum rows per file for chunking\n",
    "# BigQuery has a 1GB limit per file, but we'll use a conservative number to ensure no issues\n",
    "MAX_ROWS_PER_FILE = 500000\n",
    "\n",
    "print(f\"SQLite to BigQuery export tool initialized with export directory: {EXPORT_DIR}\")\n",
    "\n",
    "\n",
    "# Function to get a list of all tables in the database\n",
    "def get_tables(db_path):\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Query to get table names\n",
    "        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "        tables = [table[0] for table in cursor.fetchall()]\n",
    "        \n",
    "        conn.close()\n",
    "        return tables\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting tables: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# Function to get a list of columns for a table\n",
    "def get_column_info(db_path, table_name):\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Query to get column information\n",
    "        cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
    "        column_info = cursor.fetchall()\n",
    "        \n",
    "        # Extract column names and types\n",
    "        columns = [{'name': col[1], 'type': col[2]} for col in column_info]\n",
    "        \n",
    "        conn.close()\n",
    "        return columns\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting column info for table {table_name}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# Function to export table to CSV format (BigQuery compatible)\n",
    "def export_table_to_csv(db_path, table_name, export_dir, max_rows_per_file=500000):\n",
    "    \"\"\"\n",
    "    Export a table to CSV format, chunking if necessary\n",
    "    \n",
    "    Args:\n",
    "        db_path: Path to SQLite database\n",
    "        table_name: Name of table to export\n",
    "        export_dir: Directory to save exported files\n",
    "        max_rows_per_file: Maximum rows per exported file\n",
    "    \n",
    "    Returns:\n",
    "        list: Paths to exported files\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        \n",
    "        # Get total row count\n",
    "        row_count = pd.read_sql(f\"SELECT COUNT(*) FROM {table_name}\", conn).iloc[0, 0]\n",
    "        print(f\"Exporting table '{table_name}' with {row_count} rows\")\n",
    "        \n",
    "        # Calculate number of chunks needed\n",
    "        num_chunks = (row_count // max_rows_per_file) + (1 if row_count % max_rows_per_file > 0 else 0)\n",
    "        \n",
    "        exported_files = []\n",
    "        \n",
    "        for chunk_idx in tqdm(range(num_chunks), desc=f\"Exporting {table_name}\"):\n",
    "            offset = chunk_idx * max_rows_per_file\n",
    "            \n",
    "            # Read chunk of data\n",
    "            query = f\"SELECT * FROM {table_name} LIMIT {max_rows_per_file} OFFSET {offset}\"\n",
    "            df_chunk = pd.read_sql(query, conn)\n",
    "            \n",
    "            # Handle any BigQuery incompatible data types\n",
    "            for col in df_chunk.columns:\n",
    "                # Convert any complex objects to strings\n",
    "                if df_chunk[col].dtype == 'object':\n",
    "                    df_chunk[col] = df_chunk[col].apply(lambda x: str(x) if x is not None else None)\n",
    "            \n",
    "            # Create filename with chunk information if needed\n",
    "            file_suffix = f\"_part{chunk_idx+1}of{num_chunks}\" if num_chunks > 1 else \"\"\n",
    "            filename = f\"{table_name}{file_suffix}.csv\"\n",
    "            filepath = os.path.join(export_dir, filename)\n",
    "            \n",
    "            # Export to CSV\n",
    "            df_chunk.to_csv(filepath, index=False)\n",
    "            exported_files.append(filepath)\n",
    "            \n",
    "            # Force garbage collection\n",
    "            del df_chunk\n",
    "            gc.collect()\n",
    "        \n",
    "        conn.close()\n",
    "        print(f\"Exported {table_name} to {len(exported_files)} CSV file(s)\")\n",
    "        return exported_files\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting table {table_name}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# Function to export table to JSON format (BigQuery compatible)\n",
    "def export_table_to_json(db_path, table_name, export_dir, max_rows_per_file=500000):\n",
    "    \"\"\"\n",
    "    Export a table to JSON format (newline-delimited JSON), chunking if necessary\n",
    "    \n",
    "    Args:\n",
    "        db_path: Path to SQLite database\n",
    "        table_name: Name of table to export\n",
    "        export_dir: Directory to save exported files\n",
    "        max_rows_per_file: Maximum rows per exported file\n",
    "    \n",
    "    Returns:\n",
    "        list: Paths to exported files\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        \n",
    "        # Get total row count\n",
    "        row_count = pd.read_sql(f\"SELECT COUNT(*) FROM {table_name}\", conn).iloc[0, 0]\n",
    "        print(f\"Exporting table '{table_name}' with {row_count} rows\")\n",
    "        \n",
    "        # Calculate number of chunks needed\n",
    "        num_chunks = (row_count // max_rows_per_file) + (1 if row_count % max_rows_per_file > 0 else 0)\n",
    "        \n",
    "        exported_files = []\n",
    "        \n",
    "        for chunk_idx in tqdm(range(num_chunks), desc=f\"Exporting {table_name}\"):\n",
    "            offset = chunk_idx * max_rows_per_file\n",
    "            \n",
    "            # Read chunk of data\n",
    "            query = f\"SELECT * FROM {table_name} LIMIT {max_rows_per_file} OFFSET {offset}\"\n",
    "            df_chunk = pd.read_sql(query, conn)\n",
    "            \n",
    "            # Create filename with chunk information if needed\n",
    "            file_suffix = f\"_part{chunk_idx+1}of{num_chunks}\" if num_chunks > 1 else \"\"\n",
    "            filename = f\"{table_name}{file_suffix}.json\"\n",
    "            filepath = os.path.join(export_dir, filename)\n",
    "            \n",
    "            # Convert to newline-delimited JSON (for BigQuery compatibility)\n",
    "            # This creates one JSON object per line, which is what BigQuery expects\n",
    "            with open(filepath, 'w') as f:\n",
    "                f.write(df_chunk.to_json(orient='records', lines=True))\n",
    "            \n",
    "            exported_files.append(filepath)\n",
    "            \n",
    "            # Force garbage collection\n",
    "            del df_chunk\n",
    "            gc.collect()\n",
    "        \n",
    "        conn.close()\n",
    "        print(f\"Exported {table_name} to {len(exported_files)} JSON file(s)\")\n",
    "        return exported_files\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting table {table_name}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# Function to generate BigQuery schema from SQLite table\n",
    "def generate_bigquery_schema(db_path, table_name):\n",
    "    \"\"\"\n",
    "    Generate BigQuery schema JSON from SQLite table structure\n",
    "    \n",
    "    Args:\n",
    "        db_path: Path to SQLite database\n",
    "        table_name: Name of table\n",
    "        \n",
    "    Returns:\n",
    "        list: BigQuery schema definition\n",
    "    \"\"\"\n",
    "    column_info = get_column_info(db_path, table_name)\n",
    "    \n",
    "    # Map SQLite types to BigQuery types\n",
    "    type_mapping = {\n",
    "        'INTEGER': 'INTEGER',\n",
    "        'INT': 'INTEGER',\n",
    "        'SMALLINT': 'INTEGER',\n",
    "        'TINYINT': 'INTEGER',\n",
    "        'BIGINT': 'INTEGER',\n",
    "        'REAL': 'FLOAT',\n",
    "        'FLOAT': 'FLOAT',\n",
    "        'DOUBLE': 'FLOAT',\n",
    "        'TEXT': 'STRING',\n",
    "        'VARCHAR': 'STRING',\n",
    "        'CHAR': 'STRING',\n",
    "        'BLOB': 'BYTES',\n",
    "        'BOOLEAN': 'BOOLEAN',\n",
    "        'DATE': 'DATE',\n",
    "        'DATETIME': 'DATETIME',\n",
    "        'TIMESTAMP': 'TIMESTAMP'\n",
    "    }\n",
    "    \n",
    "    # Create schema\n",
    "    schema = []\n",
    "    for col in column_info:\n",
    "        # Default to STRING for unknown types\n",
    "        sqlite_type = col['type'].upper()\n",
    "        bq_type = 'STRING'\n",
    "        \n",
    "        # Check for type matches\n",
    "        for sql_type, big_query_type in type_mapping.items():\n",
    "            if sqlite_type.startswith(sql_type):\n",
    "                bq_type = big_query_type\n",
    "                break\n",
    "        \n",
    "        schema.append({\n",
    "            'name': col['name'],\n",
    "            'type': bq_type,\n",
    "            'mode': 'NULLABLE'\n",
    "        })\n",
    "    \n",
    "    return schema\n",
    "\n",
    "\n",
    "# Function to export all tables\n",
    "def export_all_tables(db_path, export_dir, file_format='csv', max_rows_per_file=500000):\n",
    "    \"\"\"\n",
    "    Export all tables from SQLite database to specified format\n",
    "    \n",
    "    Args:\n",
    "        db_path: Path to SQLite database\n",
    "        export_dir: Directory to save exports\n",
    "        file_format: Format to export ('csv' or 'json')\n",
    "        max_rows_per_file: Maximum rows per file\n",
    "        \n",
    "    Returns:\n",
    "        dict: Information about exported tables\n",
    "    \"\"\"\n",
    "    tables = get_tables(db_path)\n",
    "    \n",
    "    if not tables:\n",
    "        print(\"No tables found in the database\")\n",
    "        return {}\n",
    "    \n",
    "    print(f\"Found {len(tables)} tables: {', '.join(tables)}\")\n",
    "    \n",
    "    export_info = {}\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for table in tables:\n",
    "        table_start = time.time()\n",
    "        \n",
    "        # Create schema file for BigQuery\n",
    "        schema = generate_bigquery_schema(db_path, table)\n",
    "        schema_path = os.path.join(export_dir, f\"{table}_schema.json\")\n",
    "        \n",
    "        with open(schema_path, 'w') as f:\n",
    "            json.dump(schema, f, indent=2)\n",
    "        \n",
    "        # Export table data\n",
    "        if file_format.lower() == 'csv':\n",
    "            exported_files = export_table_to_csv(db_path, table, export_dir, max_rows_per_file)\n",
    "        else:  # json\n",
    "            exported_files = export_table_to_json(db_path, table, export_dir, max_rows_per_file)\n",
    "        \n",
    "        table_time = time.time() - table_start\n",
    "        \n",
    "        export_info[table] = {\n",
    "            'format': file_format,\n",
    "            'files': exported_files,\n",
    "            'schema_file': schema_path,\n",
    "            'file_count': len(exported_files),\n",
    "            'processing_time': f\"{table_time:.2f} seconds\"\n",
    "        }\n",
    "        \n",
    "        print(f\"Completed export of '{table}' in {table_time:.2f} seconds\")\n",
    "        \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Export completed in {total_time:.2f} seconds\")\n",
    "    \n",
    "    # Write export summary\n",
    "    summary_path = os.path.join(export_dir, 'export_summary.json')\n",
    "    with open(summary_path, 'w') as f:\n",
    "        summary = {\n",
    "            'export_date': datetime.datetime.now().isoformat(),\n",
    "            'database': db_path,\n",
    "            'format': file_format,\n",
    "            'tables': list(export_info.keys()),\n",
    "            'total_processing_time': f\"{total_time:.2f} seconds\",\n",
    "            'table_details': export_info\n",
    "        }\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    return export_info\n",
    "\n",
    "\n",
    "# Check if database file exists\n",
    "if not os.path.exists(DB_PATH):\n",
    "    print(f\"ERROR: Database file '{DB_PATH}' not found. Please update the DB_PATH variable.\")\n",
    "else:\n",
    "    print(f\"Database file '{DB_PATH}' found.\")\n",
    "    # Tables in the database\n",
    "    tables = get_tables(DB_PATH)\n",
    "    print(f\"Tables in database: {', '.join(tables)}\")\n",
    "\n",
    "# Run this cell to export all tables to CSV format (BigQuery compatible)\n",
    "export_all_tables(DB_PATH, EXPORT_DIR, file_format='csv', max_rows_per_file=MAX_ROWS_PER_FILE)\n",
    "\n",
    "# Alternatively, run this cell to export all tables to JSON format (BigQuery compatible)\n",
    "# export_all_tables(DB_PATH, EXPORT_DIR, file_format='json', max_rows_per_file=MAX_ROWS_PER_FILE)\n",
    "\n",
    "# Check the output files\n",
    "files = os.listdir(EXPORT_DIR)\n",
    "print(f\"Files in export directory ({len(files)} files):\")\n",
    "for i, file in enumerate(sorted(files)):\n",
    "    file_path = os.path.join(EXPORT_DIR, file)\n",
    "    file_size = os.path.getsize(file_path) / (1024 * 1024)  # Size in MB\n",
    "    print(f\"{i+1}. {file} ({file_size:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83508a79-4291-4769-82b5-45019319539f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
