{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02cf9303-a17d-408e-a31d-3b5437126ae0",
   "metadata": {},
   "source": [
    "# BIG BAD DATA FETCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7ab3c36-1003-473d-b43c-ec255b2f629b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import timeit\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pytz\n",
    "import os\n",
    "import sqlite3\n",
    "import json\n",
    "from pathlib import Path\n",
    "import gc\n",
    "import logging\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "\n",
    "# For visualization if needed\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Error handling and diagnostics\n",
    "import traceback\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"racing_data.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(\"RacingDB\")\n",
    "\n",
    "# Ignore specific warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "008e8f03-ec97-4211-b9de-267b5da2a48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#API Credentials\n",
    "\n",
    "# Load credentials from the text file\n",
    "with open(\"reqd_files/cred.txt\", \"r\") as file:\n",
    "    USERNAME = file.readline().strip()  # Read the first line for the username\n",
    "    PASSWORD = file.readline().strip()  # Read the second line for the password\n",
    "\n",
    "BASE_URL = \"https://api.theracingapi.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61e97796-6b1e-4b9e-9169-d6c8cf4b57aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_process(endpoint, params=None, query_number=None, max_retries=5):\n",
    "    \"\"\"\n",
    "    Make API request with error handling, retries, and rate limiting\n",
    "    \n",
    "    Args:\n",
    "        endpoint (str): API endpoint to call\n",
    "        params (dict, optional): Request parameters\n",
    "        query_number (int, optional): Query counter for logging\n",
    "        max_retries (int): Maximum number of retry attempts\n",
    "        \n",
    "    Returns:\n",
    "        dict: JSON response data or None if request failed\n",
    "    \"\"\"\n",
    "    retry_delay = 1  # Start with a 1 second delay\n",
    "    for attempt in range(max_retries):\n",
    "        start_time = timeit.default_timer()\n",
    "        \n",
    "        response = requests.get(f\"{BASE_URL}{endpoint}\", auth=(USERNAME, PASSWORD), params=params)\n",
    "        elapsed = (timeit.default_timer() - start_time) * 1000  # Convert to milliseconds\n",
    "\n",
    "        # Print statement for each query\n",
    "        if query_number and (query_number == 1 or query_number % 10 == 0):\n",
    "            print(f\"Query {query_number} duration: {elapsed:.2f} ms\")\n",
    "\n",
    "        # Handling 503 Service Unavailable\n",
    "        if response.status_code == 503:\n",
    "            print(f\"Error 503 on attempt {attempt + 1}. Retrying in {retry_delay} seconds...\")\n",
    "            time.sleep(retry_delay)\n",
    "            retry_delay *= 2  # Exponential backoff\n",
    "        else:\n",
    "            break  # Exit retry loop if response is not 503\n",
    "\n",
    "    time.sleep(0.69)  # Rate limiting - wait between requests to respect API limits\n",
    "\n",
    "    # Return data if status code is 200; otherwise, return None\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40831f8e-38a6-4ae3-8666-24ff945ac0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing API connection...\n",
      "Query 1 duration: 1544.85 ms\n",
      "Course data fetched successfully. Found 979 courses.\n"
     ]
    }
   ],
   "source": [
    "def test_api_connection():\n",
    "    \"\"\"Test API connection and retrieve course data\"\"\"\n",
    "    print(\"Testing API connection...\")\n",
    "    \n",
    "    # Define the endpoint\n",
    "    ENDPOINT = \"/v1/courses\"\n",
    "\n",
    "    # Fetch and process the data\n",
    "    courses_data = fetch_and_process(ENDPOINT, query_number=1)\n",
    "\n",
    "    # Check for data and create a DataFrame\n",
    "    if courses_data:\n",
    "        df_courses = pd.DataFrame(courses_data['courses'])\n",
    "        \n",
    "        # Save as CSV for reference\n",
    "        os.makedirs('csv_exports', exist_ok=True)\n",
    "        df_courses.to_csv('csv_exports/course_names.csv', index=False)\n",
    "        print(f\"Course data fetched successfully. Found {len(df_courses)} courses.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"Error fetching course data. Please check API credentials.\")\n",
    "        return False\n",
    "\n",
    "# Run test\n",
    "api_test_result = test_api_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b658690b-4595-4c10-a0ce-c138b62ade72",
   "metadata": {},
   "source": [
    "# Database Setup and Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1668d204-ef81-46c1-93d9-f7ad414cd685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_database(db_path='racing_data.db'):\n",
    "    \"\"\"Initialize SQLite database with schema matching the API documentation\"\"\"\n",
    "    print(f\"Setting up database at {db_path}...\")\n",
    "    \n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Create races table with fields directly from API schema\n",
    "        cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS races (\n",
    "            race_id TEXT PRIMARY KEY,\n",
    "            date TEXT,\n",
    "            region TEXT,\n",
    "            course TEXT,\n",
    "            course_id TEXT,\n",
    "            off TEXT,\n",
    "            off_dt TEXT,\n",
    "            race_name TEXT,\n",
    "            type TEXT,\n",
    "            class TEXT,\n",
    "            pattern TEXT,\n",
    "            rating_band TEXT,\n",
    "            age_band TEXT,\n",
    "            sex_rest TEXT,\n",
    "            dist TEXT,\n",
    "            dist_y TEXT,\n",
    "            dist_m TEXT,\n",
    "            dist_f TEXT,\n",
    "            going TEXT,\n",
    "            surface TEXT,\n",
    "            jumps TEXT,\n",
    "            winning_time_detail TEXT,\n",
    "            comments TEXT,\n",
    "            non_runners TEXT,\n",
    "            tote_win TEXT,\n",
    "            tote_pl TEXT,\n",
    "            tote_ex TEXT,\n",
    "            tote_csf TEXT,\n",
    "            tote_tricast TEXT,\n",
    "            tote_trifecta TEXT,\n",
    "            \n",
    "            -- Additional fields for calculations and storage\n",
    "            race_grade TEXT,\n",
    "            field_size INTEGER,\n",
    "            going_detailed TEXT,\n",
    "            rail_movements TEXT,\n",
    "            stalls TEXT,\n",
    "            weather TEXT,\n",
    "            big_race BOOLEAN,\n",
    "            is_abandoned BOOLEAN\n",
    "        )\n",
    "        ''')\n",
    "        \n",
    "        # Create runners table with fields from API schema, renaming 'or' to 'or_rating'\n",
    "        cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS runners (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            race_id TEXT,\n",
    "            horse_id TEXT,\n",
    "            horse TEXT,\n",
    "            sp TEXT,\n",
    "            sp_dec TEXT,\n",
    "            number TEXT,\n",
    "            position TEXT,\n",
    "            draw TEXT,\n",
    "            btn TEXT,\n",
    "            ovr_btn TEXT,\n",
    "            age TEXT,\n",
    "            sex TEXT,\n",
    "            weight TEXT,\n",
    "            weight_lbs TEXT,\n",
    "            headgear TEXT,\n",
    "            time TEXT,\n",
    "            or_rating TEXT,         -- Kept as or_rating to avoid SQL reserved keyword\n",
    "            rpr TEXT,\n",
    "            tsr TEXT,\n",
    "            prize TEXT,\n",
    "            jockey TEXT,\n",
    "            jockey_claim_lbs TEXT,\n",
    "            jockey_id TEXT,\n",
    "            trainer TEXT,\n",
    "            trainer_id TEXT,\n",
    "            owner TEXT,\n",
    "            owner_id TEXT,\n",
    "            sire TEXT,\n",
    "            sire_id TEXT,\n",
    "            dam TEXT,\n",
    "            dam_id TEXT,\n",
    "            damsire TEXT,\n",
    "            damsire_id TEXT,\n",
    "            comment TEXT,\n",
    "            silk_url TEXT,\n",
    "            \n",
    "            FOREIGN KEY (race_id) REFERENCES races (race_id)\n",
    "        )\n",
    "        ''')\n",
    "        \n",
    "        # Create indexes for faster queries\n",
    "        cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_race_date ON races(date)\")\n",
    "        cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_race_course ON races(course_id)\")\n",
    "        cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_runner_horse ON runners(horse_id)\")\n",
    "        cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_runner_jockey ON runners(jockey_id)\")\n",
    "        cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_runner_trainer ON runners(trainer_id)\")\n",
    "        \n",
    "        conn.commit()\n",
    "        print(\"Database tables successfully created.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up database: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        conn.close()\n",
    "    \n",
    "    print(\"Database setup complete.\")\n",
    "    return db_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826cbc4b-493e-47cb-9c87-af5921c8de08",
   "metadata": {},
   "source": [
    "### Database Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7000017-6434-4c9b-af13-c948f344aff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_existing_dates(db_path):\n",
    "    \"\"\"\n",
    "    Retrieve all distinct race dates already in the database\n",
    "    \n",
    "    Args:\n",
    "        db_path (str): Path to database\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DatetimeIndex: Dates already in database\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    try:\n",
    "        df_dates = pd.read_sql(\"SELECT DISTINCT date FROM races\", conn)\n",
    "        if not df_dates.empty:\n",
    "            return pd.DatetimeIndex(pd.to_datetime(df_dates['date']))\n",
    "        return pd.DatetimeIndex([])\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving dates: {e}\")\n",
    "        return pd.DatetimeIndex([])\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "def generate_missing_dates(start_date, end_date, existing_dates):\n",
    "    \"\"\"\n",
    "    Generate a list of dates that are not yet in the database\n",
    "    \n",
    "    Args:\n",
    "        start_date (datetime): Start date for range\n",
    "        end_date (datetime): End date for range\n",
    "        existing_dates (DatetimeIndex): Dates already in database\n",
    "        \n",
    "    Returns:\n",
    "        DatetimeIndex: Missing dates to be processed\n",
    "    \"\"\"\n",
    "    date_range = pd.date_range(start=start_date, end=end_date)\n",
    "    missing_dates = date_range.difference(existing_dates)\n",
    "    return missing_dates\n",
    "\n",
    "def optimize_database(db_path):\n",
    "    \"\"\"\n",
    "    Run VACUUM and ANALYZE commands to optimize the database\n",
    "    \n",
    "    Args:\n",
    "        db_path (str): Path to database\n",
    "    \"\"\"\n",
    "    print(\"Optimizing database...\")\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    conn.execute(\"VACUUM\")\n",
    "    conn.execute(\"ANALYZE\")\n",
    "    conn.close()\n",
    "    print(\"Database optimization complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93c6646-6558-4e8d-bc48-d7168d03549a",
   "metadata": {},
   "source": [
    "## Data Saving Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7fa4360-ca2c-4fe3-8a54-f251db250ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_database(data_to_save, db_path, endpoint_type='results'):\n",
    "    \"\"\"\n",
    "    Process API data and save to SQLite database\n",
    "    \n",
    "    Args:\n",
    "        data_to_save (list): List of race data dictionaries\n",
    "        db_path (str): Path to database\n",
    "        endpoint_type (str): Type of endpoint data (default: 'results')\n",
    "        \n",
    "    Returns:\n",
    "        int: Number of runners successfully saved\n",
    "    \"\"\"\n",
    "    if not data_to_save:\n",
    "        print(\"No data to save\")\n",
    "        return 0\n",
    "    \n",
    "    # Verify database structure\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Check if races table exists\n",
    "        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name='races'\")\n",
    "        if not cursor.fetchone():\n",
    "            print(\"Races table not found, initializing database schema...\")\n",
    "            conn.close()\n",
    "            setup_database(db_path)\n",
    "        else:\n",
    "            conn.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking database structure: {e}\")\n",
    "        setup_database(db_path)\n",
    "    \n",
    "    conn = sqlite3.connect(db_path)\n",
    "    entries_saved = 0\n",
    "    races_saved = 0\n",
    "    \n",
    "    try:\n",
    "        # Start transaction\n",
    "        conn.execute(\"BEGIN TRANSACTION\")\n",
    "        \n",
    "        for entry in data_to_save:\n",
    "            try:\n",
    "                # Extract race-level fields exactly as they appear in the schema\n",
    "                race_data = {\n",
    "                    \"race_id\": entry.get(\"race_id\"),\n",
    "                    \"date\": entry.get(\"date\"),\n",
    "                    \"region\": entry.get(\"region\"),\n",
    "                    \"course\": entry.get(\"course\"),\n",
    "                    \"course_id\": entry.get(\"course_id\"),\n",
    "                    \"off\": entry.get(\"off\"),        # Using original API field name\n",
    "                    \"off_dt\": entry.get(\"off_dt\"),\n",
    "                    \"race_name\": entry.get(\"race_name\"),\n",
    "                    \"type\": entry.get(\"type\"),\n",
    "                    \"class\": entry.get(\"class\"),\n",
    "                    \"pattern\": entry.get(\"pattern\"),\n",
    "                    \"rating_band\": entry.get(\"rating_band\"),\n",
    "                    \"age_band\": entry.get(\"age_band\"),\n",
    "                    \"sex_rest\": entry.get(\"sex_rest\"),\n",
    "                    \"dist\": entry.get(\"dist\"),      # Using original API field name\n",
    "                    \"dist_y\": entry.get(\"dist_y\"),  # Using original API field name\n",
    "                    \"dist_m\": entry.get(\"dist_m\"),  # Using original API field name\n",
    "                    \"dist_f\": entry.get(\"dist_f\"),  # Using original API field name\n",
    "                    \"going\": entry.get(\"going\"),\n",
    "                    \"surface\": entry.get(\"surface\"),\n",
    "                    \"jumps\": entry.get(\"jumps\"),\n",
    "                    \"winning_time_detail\": entry.get(\"winning_time_detail\"),\n",
    "                    \"comments\": entry.get(\"comments\"),\n",
    "                    \"non_runners\": entry.get(\"non_runners\"),\n",
    "                    \"tote_win\": entry.get(\"tote_win\"),\n",
    "                    \"tote_pl\": entry.get(\"tote_pl\"),\n",
    "                    \"tote_ex\": entry.get(\"tote_ex\"),\n",
    "                    \"tote_csf\": entry.get(\"tote_csf\"),\n",
    "                    \"tote_tricast\": entry.get(\"tote_tricast\"),\n",
    "                    \"tote_trifecta\": entry.get(\"tote_trifecta\")\n",
    "                }\n",
    "                \n",
    "                # Removed dist_m calculation code as requested\n",
    "                \n",
    "                # Create race_grade field (derived from pattern/class)\n",
    "                race_data[\"race_grade\"] = \"Unknown\"\n",
    "                if race_data.get(\"pattern\"):\n",
    "                    pattern = str(race_data[\"pattern\"]).lower()\n",
    "                    if \"group 1\" in pattern or \"grade 1\" in pattern:\n",
    "                        race_data[\"race_grade\"] = \"Pattern_1\"\n",
    "                    elif \"group 2\" in pattern or \"grade 2\" in pattern:\n",
    "                        race_data[\"race_grade\"] = \"Pattern_2\"\n",
    "                    elif \"group 3\" in pattern or \"grade 3\" in pattern:\n",
    "                        race_data[\"race_grade\"] = \"Pattern_3\"\n",
    "                    elif \"listed\" in pattern:\n",
    "                        race_data[\"race_grade\"] = \"Listed\"\n",
    "                elif race_data.get(\"class\"):\n",
    "                    try:\n",
    "                        class_num = str(race_data[\"class\"]).replace(\"Class\", \"\").strip()\n",
    "                        race_data[\"race_grade\"] = f\"Class_{class_num}\"\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "                # Add placeholders for any additional database fields not in API\n",
    "                race_data[\"field_size\"] = len(entry.get(\"runners\", []))\n",
    "                race_data[\"going_detailed\"] = race_data.get(\"going\")  # Default to regular going if detailed not available\n",
    "                race_data[\"rail_movements\"] = None\n",
    "                race_data[\"stalls\"] = None\n",
    "                race_data[\"weather\"] = None\n",
    "                race_data[\"big_race\"] = None\n",
    "                race_data[\"is_abandoned\"] = None\n",
    "                \n",
    "                # Insert race data using dynamic columns and values\n",
    "                non_null_race_data = {k: v for k, v in race_data.items() if v is not None}\n",
    "                \n",
    "                if non_null_race_data:\n",
    "                    columns = \", \".join(non_null_race_data.keys())\n",
    "                    placeholders = \", \".join([\"?\"] * len(non_null_race_data))\n",
    "                    \n",
    "                    conn.execute(\n",
    "                        f\"INSERT OR IGNORE INTO races ({columns}) VALUES ({placeholders})\",\n",
    "                        tuple(non_null_race_data.values())\n",
    "                    )\n",
    "                    races_saved += 1\n",
    "                \n",
    "                # Process runners if present\n",
    "                if \"runners\" in entry and entry[\"runners\"]:\n",
    "                    for runner in entry[\"runners\"]:\n",
    "                        # Create runner data dictionary with race_id\n",
    "                        runner_data = {\n",
    "                            \"race_id\": entry.get(\"race_id\"),\n",
    "                            \"horse_id\": runner.get(\"horse_id\"),\n",
    "                            \"horse\": runner.get(\"horse\"),\n",
    "                            \"sp\": runner.get(\"sp\"),\n",
    "                            \"sp_dec\": runner.get(\"sp_dec\"),\n",
    "                            \"number\": runner.get(\"number\"),\n",
    "                            \"position\": runner.get(\"position\"),\n",
    "                            \"draw\": runner.get(\"draw\"),\n",
    "                            \"btn\": runner.get(\"btn\"),\n",
    "                            \"ovr_btn\": runner.get(\"ovr_btn\"),\n",
    "                            \"age\": runner.get(\"age\"),\n",
    "                            \"sex\": runner.get(\"sex\"),\n",
    "                            \"weight\": runner.get(\"weight\"),\n",
    "                            \"weight_lbs\": runner.get(\"weight_lbs\"),\n",
    "                            \"headgear\": runner.get(\"headgear\"),\n",
    "                            \"time\": runner.get(\"time\"),\n",
    "                            \"or_rating\": runner.get(\"or\"),  # Only keeping this mapping (or â†’ or_rating)\n",
    "                            \"rpr\": runner.get(\"rpr\"),\n",
    "                            \"tsr\": runner.get(\"tsr\"),\n",
    "                            \"prize\": runner.get(\"prize\"),\n",
    "                            \"jockey\": runner.get(\"jockey\"),\n",
    "                            \"jockey_claim_lbs\": runner.get(\"jockey_claim_lbs\"),\n",
    "                            \"jockey_id\": runner.get(\"jockey_id\"),\n",
    "                            \"trainer\": runner.get(\"trainer\"),\n",
    "                            \"trainer_id\": runner.get(\"trainer_id\"),\n",
    "                            \"owner\": runner.get(\"owner\"),\n",
    "                            \"owner_id\": runner.get(\"owner_id\"),\n",
    "                            \"sire\": runner.get(\"sire\"),\n",
    "                            \"sire_id\": runner.get(\"sire_id\"),\n",
    "                            \"dam\": runner.get(\"dam\"),\n",
    "                            \"dam_id\": runner.get(\"dam_id\"),\n",
    "                            \"damsire\": runner.get(\"damsire\"),\n",
    "                            \"damsire_id\": runner.get(\"damsire_id\"),\n",
    "                            \"comment\": runner.get(\"comment\"),\n",
    "                            \"silk_url\": runner.get(\"silk_url\")\n",
    "                        }\n",
    "                        \n",
    "                        # Remove None values to avoid SQL errors\n",
    "                        non_null_runner_data = {k: v for k, v in runner_data.items() if v is not None}\n",
    "                        \n",
    "                        if non_null_runner_data:\n",
    "                            columns = \", \".join(non_null_runner_data.keys())\n",
    "                            placeholders = \", \".join([\"?\"] * len(non_null_runner_data))\n",
    "                            \n",
    "                            conn.execute(\n",
    "                                f\"INSERT INTO runners ({columns}) VALUES ({placeholders})\",\n",
    "                                tuple(non_null_runner_data.values())\n",
    "                            )\n",
    "                            entries_saved += 1\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error saving entry {entry.get('race_id')}: {e}\")\n",
    "                continue  # Continue with next entry\n",
    "        \n",
    "        # Commit transaction\n",
    "        conn.commit()\n",
    "        print(f\"Saved {races_saved} races and {entries_saved} runners\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Rollback on error\n",
    "        conn.rollback()\n",
    "        print(f\"Transaction failed: {e}\")\n",
    "        \n",
    "    finally:\n",
    "        conn.close()\n",
    "    \n",
    "    return entries_saved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b309c7-e4e7-47da-b266-c6a95eadf742",
   "metadata": {},
   "source": [
    "## Data Collection Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63280f0b-9fef-43d8-9f29-cf9414762b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_data_to_db(date_range, db_path, query_number=1, save_frequency=250):\n",
    "    \"\"\"\n",
    "    Query API data for a range of dates and save directly to database\n",
    "    \n",
    "    Args:\n",
    "        date_range (DatetimeIndex): Dates to query\n",
    "        db_path (str): Path to database\n",
    "        query_number (int, optional): Starting query number for logging\n",
    "        save_frequency (int): Number of entries before saving to DB\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (total_entries_saved, error_dates)\n",
    "    \"\"\"\n",
    "    ENDPOINT = \"/v1/results\"\n",
    "    data_key = 'results'\n",
    "    \n",
    "    all_data = []\n",
    "    error_dates = []\n",
    "    entries_since_save = 0\n",
    "    total_entries_saved = 0\n",
    "    \n",
    "    for single_date in date_range:\n",
    "        formatted_date = single_date.strftime(\"%Y-%m-%d\")\n",
    "        print(f\"Querying results data for date: {formatted_date}\")\n",
    "        \n",
    "        params = {\n",
    "            \"start_date\": formatted_date,\n",
    "            \"end_date\": formatted_date,\n",
    "            \"limit\": 50,\n",
    "            \"skip\": 0,\n",
    "        }\n",
    "\n",
    "        data_found = False\n",
    "        day_entries = 0\n",
    "\n",
    "        try:\n",
    "            # Fetch and process the data\n",
    "            data = fetch_and_process(ENDPOINT, params=params, query_number=query_number)\n",
    "            query_number += 1\n",
    "            \n",
    "            if not data or not data[data_key]:\n",
    "                print(f\"No results found for date: {formatted_date}\")\n",
    "                continue\n",
    "                \n",
    "            data_found = True\n",
    "            \n",
    "            # Add new results to temporary storage\n",
    "            new_entries = data[data_key]\n",
    "            all_data.extend(new_entries)\n",
    "            entries_since_save += len(new_entries)\n",
    "            day_entries += len(new_entries)\n",
    "            \n",
    "            # For results endpoint, we need to paginate\n",
    "            while len(new_entries) == 50:  # If we got the full page, try the next one\n",
    "                params['skip'] += 50\n",
    "                data = fetch_and_process(ENDPOINT, params=params, query_number=query_number)\n",
    "                query_number += 1\n",
    "                \n",
    "                if not data or not data[data_key]:\n",
    "                    break\n",
    "                    \n",
    "                new_entries = data[data_key]\n",
    "                all_data.extend(new_entries)\n",
    "                entries_since_save += len(new_entries)\n",
    "                day_entries += len(new_entries)\n",
    "            \n",
    "            # Save progress if we've accumulated enough new entries\n",
    "            if entries_since_save >= save_frequency:\n",
    "                saved_count = save_to_database(all_data[-entries_since_save:], db_path, 'results')\n",
    "                total_entries_saved += saved_count\n",
    "                print(f\"Progress saved: {saved_count} new runners written to database\")\n",
    "                all_data = []  # Clear memory after saving\n",
    "                entries_since_save = 0  # Reset counter after saving\n",
    "                \n",
    "                # Force garbage collection\n",
    "                gc.collect()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred on {formatted_date}: {e}\")\n",
    "            error_dates.append(formatted_date)\n",
    "            traceback.print_exc()  # Print full traceback for debugging\n",
    "\n",
    "        if data_found:\n",
    "            print(f\"Results data found for date: {formatted_date} - {day_entries} entries\")\n",
    "\n",
    "    # Save any remaining data\n",
    "    if entries_since_save > 0:\n",
    "        saved_count = save_to_database(all_data, db_path, 'results')\n",
    "        total_entries_saved += saved_count\n",
    "        print(f\"Final save: {saved_count} remaining runners written to database\")\n",
    "\n",
    "    return total_entries_saved, error_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78899426-5160-4638-9059-2ab1643a387f",
   "metadata": {},
   "source": [
    "## Main Execution Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18f6bd23-3bba-42ee-8e25-78d691291c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_racing_database(db_path='racing_data.db', default_earliest_date=None):\n",
    "    \"\"\"\n",
    "    Main function to update the racing database with new data\n",
    "    \n",
    "    Args:\n",
    "        db_path (str): Path to database\n",
    "        default_earliest_date (datetime, optional): Earliest date to start from if not specified\n",
    "        \n",
    "    Returns:\n",
    "        dict: Summary of data collection results\n",
    "    \"\"\"\n",
    "    if default_earliest_date is None:\n",
    "        default_earliest_date = pd.to_datetime(\"2020-01-01\")\n",
    "    \n",
    "    print(f\"Starting database update process...\")\n",
    "    print(f\"Default earliest date: {default_earliest_date.strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    # Ensure database is properly set up before any operations\n",
    "    setup_database(db_path)\n",
    "    \n",
    "    # Get existing dates from database\n",
    "    existing_dates = get_existing_dates(db_path)\n",
    "    \n",
    "    if len(existing_dates) > 0:\n",
    "        earliest_date = existing_dates.min()\n",
    "        latest_date = existing_dates.max()\n",
    "        \n",
    "        print(f\"Database contains {len(existing_dates)} dates\")\n",
    "        print(f\"Earliest date in DB: {earliest_date.strftime('%Y-%m-%d')}\")\n",
    "        print(f\"Latest date in DB: {latest_date.strftime('%Y-%m-%d')}\")\n",
    "        \n",
    "        # Generate missing dates\n",
    "        missing_before = generate_missing_dates(default_earliest_date, earliest_date - pd.Timedelta(days=1), existing_dates)\n",
    "        start_date = latest_date + pd.Timedelta(days=1)\n",
    "        end_date = pd.to_datetime(\"today\") - pd.Timedelta(days=1)\n",
    "        missing_after = generate_missing_dates(start_date, end_date, existing_dates)\n",
    "        \n",
    "        # Check for missing dates in the middle\n",
    "        all_expected_dates = pd.date_range(earliest_date, latest_date)\n",
    "        missing_middle = all_expected_dates.difference(existing_dates)\n",
    "        \n",
    "        # Process missing dates\n",
    "        total_entries = 0\n",
    "        all_errors = []\n",
    "        \n",
    "        if not missing_before.empty:\n",
    "            print(f\"\\nQuerying {len(missing_before)} missing dates before the DB start\")\n",
    "            entries_before, errors_before = query_data_to_db(missing_before, db_path)\n",
    "            total_entries += entries_before\n",
    "            all_errors.extend(errors_before)\n",
    "            \n",
    "        if not missing_middle.empty:\n",
    "            print(f\"\\nQuerying {len(missing_middle)} missing dates in the middle of the DB\")\n",
    "            entries_middle, errors_middle = query_data_to_db(missing_middle, db_path)\n",
    "            total_entries += entries_middle\n",
    "            all_errors.extend(errors_middle)\n",
    "            \n",
    "        if not missing_after.empty:\n",
    "            print(f\"\\nQuerying {len(missing_after)} missing dates after the DB end\")\n",
    "            entries_after, errors_after = query_data_to_db(missing_after, db_path)\n",
    "            total_entries += entries_after\n",
    "            all_errors.extend(errors_after)\n",
    "        \n",
    "        results = {\n",
    "            'total_entries': total_entries,\n",
    "            'errors': all_errors\n",
    "        }\n",
    "        \n",
    "    else:\n",
    "        # DB is empty, start from scratch\n",
    "        print(f\"Database is empty. Starting fresh data collection.\")\n",
    "        all_dates = pd.date_range(default_earliest_date, pd.to_datetime(\"today\") - pd.Timedelta(days=1))\n",
    "        print(f\"Collecting data for {len(all_dates)} dates from {default_earliest_date.strftime('%Y-%m-%d')} to {pd.to_datetime('today').strftime('%Y-%m-%d')}\")\n",
    "        \n",
    "        total, errors = query_data_to_db(all_dates, db_path)\n",
    "        \n",
    "        results = {\n",
    "            'total_entries': total,\n",
    "            'errors': errors\n",
    "        }\n",
    "    \n",
    "    # Optimize database after updates\n",
    "    optimize_database(db_path)\n",
    "    \n",
    "    # Provide summary\n",
    "    print(\"\\n=== Database Update Summary ===\")\n",
    "    print(f\"Results: {results['total_entries']} entries added, {len(results['errors'])} dates with errors\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Updated function to get existing dates focused only on results\n",
    "def get_existing_dates(db_path):\n",
    "    \"\"\"\n",
    "    Retrieve all distinct race dates already in the database for results\n",
    "    \n",
    "    Args:\n",
    "        db_path (str): Path to database\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DatetimeIndex: Dates already in database\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    try:\n",
    "        # For results, check races table with valid position data in runners\n",
    "        query = \"\"\"\n",
    "        SELECT DISTINCT r.date FROM races r\n",
    "        JOIN runners ru ON r.race_id = ru.race_id\n",
    "        WHERE ru.position IS NOT NULL\n",
    "        \"\"\"\n",
    "        \n",
    "        df_dates = pd.read_sql(query, conn)\n",
    "        if not df_dates.empty:\n",
    "            return pd.DatetimeIndex(pd.to_datetime(df_dates['date']))\n",
    "        return pd.DatetimeIndex([])\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving dates: {e}\")\n",
    "        return pd.DatetimeIndex([])\n",
    "    finally:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a750c0a-b402-436a-a2c1-7d1d6fc4592a",
   "metadata": {},
   "source": [
    "### Execute Database Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43f37bfa-52fc-4786-bcc1-94bb64c3cf2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "API connection successful. Proceeding with database update.\n",
      "Setting up database at racing_data.db...\n",
      "Database tables successfully created.\n",
      "Database setup complete.\n",
      "Starting database update process...\n",
      "Default earliest date: 2010-01-01\n",
      "Setting up database at racing_data.db...\n",
      "Database tables successfully created.\n",
      "Database setup complete.\n",
      "Database contains 5532 dates\n",
      "Earliest date in DB: 2010-01-01\n",
      "Latest date in DB: 2025-03-01\n",
      "\n",
      "Querying 7 missing dates in the middle of the DB\n",
      "Querying results data for date: 2010-02-02\n",
      "Query 1 duration: 1698.14 ms\n",
      "Results data found for date: 2010-02-02 - 14 entries\n",
      "Querying results data for date: 2018-12-24\n",
      "No results found for date: 2018-12-24\n",
      "Querying results data for date: 2018-12-25\n",
      "No results found for date: 2018-12-25\n",
      "Querying results data for date: 2020-04-02\n",
      "No results found for date: 2020-04-02\n",
      "Querying results data for date: 2020-04-03\n",
      "No results found for date: 2020-04-03\n",
      "Querying results data for date: 2020-04-06\n",
      "No results found for date: 2020-04-06\n",
      "Querying results data for date: 2024-12-25\n",
      "No results found for date: 2024-12-25\n",
      "Saved 14 races and 131 runners\n",
      "Final save: 131 remaining runners written to database\n",
      "\n",
      "Querying 11 missing dates after the DB end\n",
      "Querying results data for date: 2025-03-02\n",
      "Query 1 duration: 2424.34 ms\n",
      "Results data found for date: 2025-03-02 - 50 entries\n",
      "Querying results data for date: 2025-03-03\n",
      "Results data found for date: 2025-03-03 - 30 entries\n",
      "Querying results data for date: 2025-03-04\n",
      "Results data found for date: 2025-03-04 - 38 entries\n",
      "Querying results data for date: 2025-03-05\n",
      "Results data found for date: 2025-03-05 - 32 entries\n",
      "Querying results data for date: 2025-03-06\n",
      "Results data found for date: 2025-03-06 - 35 entries\n",
      "Querying results data for date: 2025-03-07\n",
      "Results data found for date: 2025-03-07 - 46 entries\n",
      "Querying results data for date: 2025-03-08\n",
      "Saved 310 races and 2972 runners\n",
      "Progress saved: 2972 new runners written to database\n",
      "Results data found for date: 2025-03-08 - 79 entries\n",
      "Querying results data for date: 2025-03-09\n",
      "Query 10 duration: 2225.76 ms\n",
      "Results data found for date: 2025-03-09 - 48 entries\n",
      "Querying results data for date: 2025-03-10\n",
      "Results data found for date: 2025-03-10 - 29 entries\n",
      "Querying results data for date: 2025-03-11\n",
      "Results data found for date: 2025-03-11 - 37 entries\n",
      "Querying results data for date: 2025-03-12\n",
      "Results data found for date: 2025-03-12 - 45 entries\n",
      "Saved 159 races and 1608 runners\n",
      "Final save: 1608 remaining runners written to database\n",
      "Optimizing database...\n",
      "Database optimization complete.\n",
      "\n",
      "=== Database Update Summary ===\n",
      "Results: 4711 entries added, 0 dates with errors\n",
      "\n",
      "Database Update Complete\n"
     ]
    }
   ],
   "source": [
    "# Modify the execution code at the bottom\n",
    "if __name__ == \"__main__\":\n",
    "    if api_test_result:\n",
    "        print(\"\\nAPI connection successful. Proceeding with database update.\")\n",
    "        \n",
    "        # Initialize the database explicitly\n",
    "        db_path = 'racing_data.db'\n",
    "        setup_database(db_path)\n",
    "        \n",
    "        # Set start date (change as needed)\n",
    "        start_date = pd.to_datetime(\"2010-01-01\")  # More recent start date for testing\n",
    "        \n",
    "        # Update racing database with results endpoint only\n",
    "        results = update_racing_database(\n",
    "            db_path=db_path,\n",
    "            default_earliest_date=start_date\n",
    "        )\n",
    "        \n",
    "        print(\"\\nDatabase Update Complete\")\n",
    "        \n",
    "        # Process errors\n",
    "        if results['errors']:\n",
    "            print(f\"\\nErrors encountered on {len(results['errors'])} dates:\")\n",
    "            for date in results['errors'][:10]:  # Show first 10 error dates\n",
    "                print(f\"  - {date}\")\n",
    "            if len(results['errors']) > 10:\n",
    "                print(f\"  ... and {len(results['errors']) - 10} more\")\n",
    "                \n",
    "            # Save error dates to file for future processing\n",
    "            with open('error_dates_results.json', 'w') as f:\n",
    "                json.dump({'error_dates': [d for d in results['errors']]}, f)\n",
    "    else:\n",
    "        print(\"\\nAPI connection failed. Please check your credentials and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde085db-b638-412e-b649-c741a972421e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
