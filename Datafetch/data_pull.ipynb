{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02cf9303-a17d-408e-a31d-3b5437126ae0",
   "metadata": {},
   "source": [
    "# BIG BAD DATA FETCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7ab3c36-1003-473d-b43c-ec255b2f629b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import timeit\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pytz\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import gc\n",
    "\n",
    "# Joblib for model persistence\n",
    "from joblib import dump, load\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "008e8f03-ec97-4211-b9de-267b5da2a48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#API Credentials\n",
    "\n",
    "# Load credentials from the text file\n",
    "with open(\"reqd_files/cred.txt\", \"r\") as file:\n",
    "    USERNAME = file.readline().strip()  # Read the first line for the username\n",
    "    PASSWORD = file.readline().strip()  # Read the second line for the password\n",
    "\n",
    "BASE_URL = \"https://api.theracingapi.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61e97796-6b1e-4b9e-9169-d6c8cf4b57aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_process(endpoint, params=None, query_number=None, max_retries=5):\n",
    "    retry_delay = 1  # Start with a 5 second delay\n",
    "    for attempt in range(max_retries):\n",
    "        start_time = timeit.default_timer()\n",
    "        \n",
    "        response = requests.get(f\"{BASE_URL}{endpoint}\", auth=(USERNAME, PASSWORD), params=params)\n",
    "        elapsed = (timeit.default_timer() - start_time) * 1000  # Convert to milliseconds\n",
    "\n",
    "        # Print statement for each query\n",
    "        if query_number and (query_number == 1 or query_number % 10 == 0):\n",
    "            print(f\"Query {query_number} duration: {elapsed:.2f} ms\")\n",
    "\n",
    "        # Handling 503 Service Unavailable\n",
    "        if response.status_code == 503:\n",
    "            print(f\"Error 503 on attempt {attempt + 1}. Retrying in {retry_delay} seconds...\")\n",
    "            time.sleep(retry_delay)\n",
    "            retry_delay *= 2  # Exponential backoff\n",
    "        else:\n",
    "            break  # Exit retry loop if response is not 503\n",
    "\n",
    "    time.sleep(0.69)  # Ensure not to exceed the rate limit of the API -- can go to 0.5\n",
    "\n",
    "    # Return data if status code is 200; otherwise, return None\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40831f8e-38a6-4ae3-8666-24ff945ac0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1 duration: 1968.26 ms\n",
      "Course data fetched and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define the endpoint\n",
    "ENDPOINT = \"/v1/courses\"\n",
    "\n",
    "# Fetch and process the data\n",
    "courses_data = fetch_and_process(ENDPOINT, query_number=1)\n",
    "\n",
    "# Check for data and create a DataFrame\n",
    "if courses_data:\n",
    "    df_courses = pd.DataFrame(courses_data['courses'])\n",
    "    df_courses.to_csv('csv_exports/course_names.csv', index=False)\n",
    "    print(\"Course data fetched and saved successfully.\")\n",
    "else:\n",
    "    print(\"Error fetching data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b658690b-4595-4c10-a0ce-c138b62ade72",
   "metadata": {},
   "source": [
    "# Below is the big block of code that pulls the entire history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1668d204-ef81-46c1-93d9-f7ad414cd685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import datetime\n",
    "\n",
    "# # Initialize an empty DataFrame to store the appended data\n",
    "# df_list_racecards_pro = pd.DataFrame()\n",
    "\n",
    "# # Define the start date as yesterday and the end date as 6 days from today\n",
    "# start_date = datetime.datetime.today() - datetime.timedelta(days=1)\n",
    "# end_date = start_date + datetime.timedelta(days=7)\n",
    "\n",
    "# # Calculate the number of days between start date and end date\n",
    "# num_days = (end_date - start_date).days\n",
    "\n",
    "# # Define the endpoint\n",
    "# ENDPOINT = \"/v1/racecards/pro\"\n",
    "\n",
    "# # Iterate through each day from today until the next 7 days\n",
    "# for i in range(num_days + 1):  # +1 to include end_date\n",
    "#     date = (start_date + datetime.timedelta(days=i)).strftime('%Y-%m-%d')\n",
    "#     print(f\"Fetching data for date: {date}\")\n",
    "\n",
    "#     try:\n",
    "#         data = fetch_and_process(ENDPOINT, params={\"date\": date}, query_number=i+1)\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred while fetching data for {date}: {e}\")\n",
    "#         continue\n",
    "\n",
    "#     if data:\n",
    "#         # Directly use the racecards data, as we're not exploding the runners\n",
    "#         temp_df = pd.DataFrame(data['racecards'])\n",
    "        \n",
    "#         # Concatenate the new DataFrame with the main DataFrame\n",
    "#         df_list_racecards_pro = pd.concat([df_list_racecards_pro, temp_df], ignore_index=True)\n",
    "#     else:\n",
    "#         print(f\"No data returned for date: {date}\")\n",
    "\n",
    "# # Save the DataFrame to a CSV file\n",
    "# df_list_racecards_pro.to_csv('csv_exports/future_racecards_pro.csv', index=False)\n",
    "# print(\"Racecards pro data for the next 7 days have been fetched and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bb193e8-fd2f-44e2-bec0-6f23fc7a67a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from pathlib import Path\n",
    "\n",
    "# # Define the path to the CSV file\n",
    "# csv_path = Path('csv_exports/bulk_results.csv')\n",
    "\n",
    "# # Check if the CSV file exists\n",
    "# if csv_path.exists():\n",
    "#     # Read the CSV file\n",
    "#     existing_data = pd.read_csv(csv_path)\n",
    "#     print(\"CSV file loaded successfully.\")\n",
    "\n",
    "#     # Convert the 'date' column to datetime format\n",
    "#     existing_data['date'] = pd.to_datetime(existing_data['date'])\n",
    "\n",
    "#     # Find the earliest date\n",
    "#     earliest_date = existing_data['date'].min()\n",
    "#     print(f\"Earliest date in the CSV: {earliest_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "#     # Find the latest date\n",
    "#     latest_date = existing_data['date'].max()\n",
    "#     print(f\"Latest date in the CSV: {latest_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "#     # Set the start date to the day after the latest date\n",
    "#     start_date = latest_date + pd.Timedelta(days=1)\n",
    "#     # Set the end date to yesterday's date\n",
    "#     end_date = pd.to_datetime(\"today\") - pd.Timedelta(days=1)\n",
    "    \n",
    "#     print(f\"Start date: {start_date.strftime('%Y-%m-%d')}\")\n",
    "#     print(f\"End date: {end_date.strftime('%Y-%m-%d')}\")\n",
    "# else:\n",
    "#     print(\"CSV file does not exist.\")\n",
    "    \n",
    "#     # Set the start date to a default date or a designated start date\n",
    "#     default_start_date = pd.to_datetime(\"2015-01-01\")\n",
    "    \n",
    "#     # You can designate your own start date here\n",
    "#     designated_start_date = input(\"Enter a start date (YYYY-MM-DD) or press Enter to use default (2015-01-01): \")\n",
    "    \n",
    "#     if designated_start_date:\n",
    "#         try:\n",
    "#             start_date = pd.to_datetime(designated_start_date)\n",
    "#             print(f\"Using designated start date: {start_date.strftime('%Y-%m-%d')}\")\n",
    "#         except ValueError:\n",
    "#             print(\"Invalid date format. Using default start date.\")\n",
    "#             start_date = default_start_date\n",
    "#             print(f\"Default start date: {start_date.strftime('%Y-%m-%d')}\")\n",
    "#     else:\n",
    "#         start_date = default_start_date\n",
    "#         print(f\"Default start date: {start_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "#     # Set the end date to yesterday's date\n",
    "#     end_date = pd.to_datetime(\"today\") - pd.Timedelta(days=1)\n",
    "    \n",
    "#     print(f\"End date: {end_date.strftime('%Y-%m-%d')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93c6646-6558-4e8d-bc48-d7168d03549a",
   "metadata": {},
   "source": [
    "## This one below is to start again .. just play with the start and end date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7fa4360-ca2c-4fe3-8a54-f251db250ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Define the endpoint\n",
    "# ENDPOINT = \"/v1/results\"\n",
    "\n",
    "# # Generating a list of dates\n",
    "# start_date = datetime.datetime(2010, 1, 1)\n",
    "# end_date = datetime.datetime(2010, 1, 15)\n",
    "# date_range = pd.date_range(start_date, end_date)\n",
    "\n",
    "# # API Call for Each Day\n",
    "# all_data = []\n",
    "# query_number = 1\n",
    "# error_dates = []  # List to store dates with errors\n",
    "\n",
    "# for single_date in date_range:\n",
    "#     formatted_date = single_date.strftime(\"%Y-%m-%d\")\n",
    "#     print(f\"Querying data for date: {formatted_date}\")  # Print the date being queried\n",
    "#     params = {\n",
    "#         \"start_date\": formatted_date,\n",
    "#         \"end_date\": formatted_date,\n",
    "#         \"limit\": 50,\n",
    "#         \"skip\": 0,\n",
    "#     }\n",
    "    \n",
    "#     data_found = False  # Flag to check if data was found for the date\n",
    "\n",
    "#     while True:\n",
    "#         try:\n",
    "#             # Fetch and process the data\n",
    "#             data = fetch_and_process(ENDPOINT, params=params, query_number=query_number)\n",
    "#             query_number += 1\n",
    "#             if not data or not data['results']:\n",
    "#                 break\n",
    "#             data_found = True  # Set flag to True as data was found\n",
    "#             all_data.extend(data['results'])\n",
    "#             params['skip'] += 50  # Increment the skip parameter for the next iteration\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error occurred on {formatted_date}: {e}\")\n",
    "#             error_dates.append(formatted_date)  # Add the date to the error list\n",
    "#             break  # Break the inner loop if an error occurs\n",
    "\n",
    "#     if data_found:\n",
    "#         print(f\"Data found for date: {formatted_date}\")\n",
    "#     else:\n",
    "#         print(f\"No data found for date: {formatted_date}\")\n",
    "\n",
    "# # Assuming all_data is a list of dictionaries with the new data\n",
    "# try:\n",
    "#     if all_data:  # Check if there is any new data to append\n",
    "#         # Convert the list of dictionaries to a DataFrame\n",
    "#         bulk_results = pd.DataFrame(all_data)\n",
    "#         if not bulk_results.empty and 'runners' in bulk_results.columns:\n",
    "#             bulk_results = bulk_results.explode('runners')\n",
    "#             runners_data = bulk_results['runners'].apply(pd.Series)\n",
    "#             bulk_results = pd.concat([bulk_results.drop(columns='runners'), runners_data], axis=1)\n",
    "\n",
    "#         # Define the path to the CSV file\n",
    "#         csv_path = Path('csv_exports/bulk_results.csv')\n",
    "\n",
    "#         # Check if the CSV file exists to determine if we need to write headers\n",
    "#         file_exists = csv_path.exists()\n",
    "\n",
    "#         # Export to CSV, appending if file exists, including header if file does not exist\n",
    "#         bulk_results.to_csv(csv_path, mode='a', header=not file_exists, index=False)\n",
    "#         print(\"Data appended to CSV successfully.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error during data processing or export: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b309c7-e4e7-47da-b266-c6a95eadf742",
   "metadata": {},
   "source": [
    "## This one below creates a sample csv from the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63280f0b-9fef-43d8-9f29-cf9414762b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Configuration for sampling\n",
    "# BLOCK_SIZE = 7      # Number of rows in each block\n",
    "# NUM_BLOCKS = 10      # Number of blocks to sample\n",
    "\n",
    "# # Read the main CSV file\n",
    "# csv_path = Path('csv_exports/bulk_results.csv')\n",
    "# df = pd.read_csv(csv_path)\n",
    "\n",
    "# # Calculate how many complete blocks we can have\n",
    "# total_blocks = len(df) // BLOCK_SIZE\n",
    "\n",
    "# # Ensure we're not trying to sample more blocks than exist\n",
    "# if NUM_BLOCKS > total_blocks:\n",
    "#     print(f\"Warning: Requested {NUM_BLOCKS} blocks but only {total_blocks} exist. Using maximum available.\")\n",
    "#     NUM_BLOCKS = total_blocks\n",
    "\n",
    "# # Generate random block starting points\n",
    "# # This randomly picks NUM_BLOCKS numbers from range(total_blocks)\n",
    "# # Each number represents the start of a block\n",
    "# print(\"Block starting positions chosen:\")\n",
    "# random_blocks = random.sample(range(total_blocks), NUM_BLOCKS)\n",
    "# for i, block_start in enumerate(sorted(random_blocks)):\n",
    "#     print(f\"Block {i+1} starts at row {block_start * BLOCK_SIZE}\")\n",
    "\n",
    "# # Initialize list to store selected rows\n",
    "# selected_rows = []\n",
    "\n",
    "# # Get blocks for each randomly selected starting point\n",
    "# for block_start in random_blocks:\n",
    "#     start_idx = block_start * BLOCK_SIZE\n",
    "#     end_idx = start_idx + BLOCK_SIZE\n",
    "#     selected_rows.extend(list(range(start_idx, end_idx)))\n",
    "\n",
    "# # Create new DataFrame with selected rows\n",
    "# example_df = df.iloc[selected_rows].copy()\n",
    "\n",
    "# # Save to new CSV file with explicit header\n",
    "# example_df.to_csv('csv_exports/example_csv.csv', index=False, header=True)\n",
    "\n",
    "# print(f\"\\nCreated example CSV with {len(example_df)} rows\")\n",
    "# print(f\"({NUM_BLOCKS} blocks of {BLOCK_SIZE} rows each)\")\n",
    "# print(f\"Original CSV had {len(df)} rows\")\n",
    "# print(\"Column headers have been included in the example CSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78899426-5160-4638-9059-2ab1643a387f",
   "metadata": {},
   "source": [
    "## This below is to find unseen date and append, rather than start again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18f6bd23-3bba-42ee-8e25-78d691291c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file does not exist. Please create it before running the script.\n"
     ]
    }
   ],
   "source": [
    "# Function to generate a list of missing dates\n",
    "def generate_missing_dates(start_date, end_date, existing_dates):\n",
    "    date_range = pd.date_range(start=start_date, end=end_date)\n",
    "    missing_dates = date_range.difference(existing_dates)\n",
    "    return missing_dates\n",
    "\n",
    "# Define the endpoint\n",
    "ENDPOINT = \"/v1/results\"\n",
    "\n",
    "# Function to query data via API for a date range with regular saving\n",
    "def query_data(date_range, query_number=1, save_frequency=2500, csv_path=None):\n",
    "    all_data = []\n",
    "    error_dates = []\n",
    "    entries_since_save = 0\n",
    "    \n",
    "    for single_date in date_range:\n",
    "        formatted_date = single_date.strftime(\"%Y-%m-%d\")\n",
    "        print(f\"Querying data for date: {formatted_date}\")\n",
    "        params = {\n",
    "            \"start_date\": formatted_date,\n",
    "            \"end_date\": formatted_date,\n",
    "            \"limit\": 50,\n",
    "            \"skip\": 0,\n",
    "        }\n",
    "\n",
    "        data_found = False\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                # Fetch and process the data\n",
    "                data = fetch_and_process(ENDPOINT, params=params, query_number=query_number)\n",
    "                query_number += 1\n",
    "                \n",
    "                if not data or not data['results']:\n",
    "                    break\n",
    "                    \n",
    "                data_found = True\n",
    "                \n",
    "                # Add new results and track how many were added\n",
    "                new_entries = data['results']\n",
    "                all_data.extend(new_entries)\n",
    "                entries_since_save += len(new_entries)\n",
    "                \n",
    "                # Save progress if we've accumulated enough new entries\n",
    "                if csv_path and entries_since_save >= save_frequency:\n",
    "                    save_progress_to_csv(all_data[-entries_since_save:], csv_path)  # Only save the new entries\n",
    "                    print(f\"Progress saved: {entries_since_save} new entries written to CSV\")\n",
    "                    entries_since_save = 0  # Reset counter after saving\n",
    "                \n",
    "                params['skip'] += 50  # Increment the skip parameter for the next iteration\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error occurred on {formatted_date}: {e}\")\n",
    "                error_dates.append(formatted_date)\n",
    "                break\n",
    "\n",
    "        if data_found:\n",
    "            print(f\"Data found for date: {formatted_date}\")\n",
    "        else:\n",
    "            print(f\"No data found for date: {formatted_date}\")\n",
    "\n",
    "    # Save any remaining data that didn't reach the save frequency threshold\n",
    "    if csv_path and entries_since_save > 0:\n",
    "        save_progress_to_csv(all_data[-entries_since_save:], csv_path)  # Only save the new entries\n",
    "        print(f\"Final save: {entries_since_save} remaining entries written to CSV\")\n",
    "\n",
    "    return all_data, error_dates\n",
    "\n",
    "# Function to save progress to CSV\n",
    "def save_progress_to_csv(data_to_save, csv_path):\n",
    "    # Process the accumulated data\n",
    "    if not data_to_save:\n",
    "        print(\"No data to save\")\n",
    "        return\n",
    "        \n",
    "    bulk_results = pd.DataFrame(data_to_save)\n",
    "    \n",
    "    if 'runners' in bulk_results.columns:\n",
    "        bulk_results = bulk_results.explode('runners')\n",
    "        runners_data = bulk_results['runners'].apply(pd.Series)\n",
    "        bulk_results = pd.concat([bulk_results.drop(columns='runners'), runners_data], axis=1)\n",
    "\n",
    "    # Check if file exists to determine if we need headers\n",
    "    file_exists = Path(csv_path).exists()\n",
    "    \n",
    "    # Export to CSV, appending data\n",
    "    bulk_results.to_csv(csv_path, mode='a', header=not file_exists, index=False)\n",
    "\n",
    "# Define the path to the CSV file\n",
    "csv_path = Path('csv_exports/bulk_results.csv')\n",
    "\n",
    "# Define the default earliest date\n",
    "default_earliest_date = pd.to_datetime(\"2015-01-01\")\n",
    "\n",
    "# Check if the CSV file exists\n",
    "if csv_path.exists():\n",
    "    # Read the CSV file\n",
    "    existing_data = pd.read_csv(csv_path)\n",
    "    print(\"CSV file loaded successfully.\")\n",
    "\n",
    "    # Convert the 'date' column to datetime format\n",
    "    existing_data['date'] = pd.to_datetime(existing_data['date'])\n",
    "\n",
    "    # Find the earliest and latest dates in the CSV file\n",
    "    earliest_date = existing_data['date'].min()\n",
    "    latest_date = existing_data['date'].max()\n",
    "\n",
    "    print(f\"Earliest date in the CSV: {earliest_date.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Latest date in the CSV: {latest_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "    # Generate missing dates\n",
    "    missing_before = generate_missing_dates(default_earliest_date, earliest_date - pd.Timedelta(days=1), existing_data['date'])\n",
    "    start_date = latest_date + pd.Timedelta(days=1)\n",
    "    end_date = pd.to_datetime(\"today\") - pd.Timedelta(days=1)\n",
    "    missing_after = generate_missing_dates(start_date, end_date, existing_data['date'])\n",
    "\n",
    "    # Query missing data with regular saving (every 2500 entries)\n",
    "    all_data_before, errors_before = [], []\n",
    "    all_data_after, errors_after = [], []\n",
    "\n",
    "    if not missing_before.empty:\n",
    "        print(f\"Querying missing dates before the CSV start\")\n",
    "        all_data_before, errors_before = query_data(\n",
    "            missing_before, \n",
    "            query_number=1,\n",
    "            save_frequency=2500,  # Save every 2500 entries\n",
    "            csv_path=csv_path\n",
    "        )\n",
    "\n",
    "    if not missing_after.empty:\n",
    "        print(f\"Querying missing dates after the CSV end\")\n",
    "        all_data_after, errors_after = query_data(\n",
    "            missing_after, \n",
    "            query_number=len(all_data_before) + 1,\n",
    "            save_frequency=2500,  # Save every 2500 entries\n",
    "            csv_path=csv_path\n",
    "        )\n",
    "\n",
    "    # Report statistics\n",
    "    print(f\"Total entries added: {len(all_data_before) + len(all_data_after)}\")\n",
    "    if errors_before or errors_after:\n",
    "        print(f\"Errors encountered on dates: {errors_before + errors_after}\")\n",
    "\n",
    "else:\n",
    "    print(\"CSV file does not exist. Please create it before running the script.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30649415-5cfa-4689-99a7-c2dbd37f173d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code to check whether the CSV has changed strucutre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43f37bfa-52fc-4786-bcc1-94bb64c3cf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "\n",
    "# def check_csv_structure(filename, expected_fields_count=63):\n",
    "#     with open(filename, 'r', newline='') as csvfile:\n",
    "#         reader = csv.reader(csvfile)\n",
    "#         header = next(reader)\n",
    "#         if len(header) != expected_fields_count:\n",
    "#             print(f\"Header changed: Expected {expected_fields_count} fields but found {len(header)}.\")\n",
    "#         else:\n",
    "#             print(\"Header matches expected field count.\")\n",
    "            \n",
    "# # Replace 'data.csv' with your actual CSV file path\n",
    "# check_csv_structure('csv_exports/bulk_results.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d697af0-d3cf-4f5c-920b-6b5254c2d3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_incorrect_lines(filename, expected_fields_count=63):\n",
    "#     bad_lines = []\n",
    "#     with open(filename, 'r', newline='') as csvfile:\n",
    "#         reader = csv.reader(csvfile)\n",
    "#         for i, row in enumerate(reader, start=1):\n",
    "#             if len(row) != expected_fields_count:\n",
    "#                 bad_lines.append(i)\n",
    "#     return bad_lines\n",
    "\n",
    "# bad_lines = find_incorrect_lines('csv_exports/bulk_results.csv')\n",
    "# print(\"Lines with incorrect field count:\", bad_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d8d83b4-b036-466f-8380-332f0939ca33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# import linecache\n",
    "\n",
    "# def export_context(filename, offending_line, context=10, output_file='offending_context.csv'):\n",
    "#     # Read the header row\n",
    "#     with open(filename, 'r', newline='') as f:\n",
    "#         reader = csv.reader(f)\n",
    "#         header = next(reader)\n",
    "    \n",
    "#     # Determine context range. We always include the header (line 1) separately.\n",
    "#     # Since the header is line 1, our data lines start at line 2.\n",
    "#     start_line = max(offending_line - context, 2)\n",
    "#     end_line = offending_line + context\n",
    "    \n",
    "#     # Read the context lines using linecache.\n",
    "#     lines = []\n",
    "#     for i in range(start_line, end_line + 1):\n",
    "#         line = linecache.getline(filename, i)\n",
    "#         if line:\n",
    "#             lines.append(line)\n",
    "    \n",
    "#     # Parse each context line using csv.reader\n",
    "#     context_data = []\n",
    "#     for l in lines:\n",
    "#         # Parse each line; csv.reader returns an iterator so we grab the first row.\n",
    "#         row = next(csv.reader([l]))\n",
    "#         context_data.append(row)\n",
    "    \n",
    "#     # Determine the maximum column count between header and any context row.\n",
    "#     max_columns = max(len(header), *(len(row) for row in context_data))\n",
    "    \n",
    "#     # If header is shorter than the maximum, extend it with empty strings.\n",
    "#     if len(header) < max_columns:\n",
    "#         header.extend([''] * (max_columns - len(header)))\n",
    "    \n",
    "#     # Also, pad each context row so they all have max_columns fields.\n",
    "#     for row in context_data:\n",
    "#         if len(row) < max_columns:\n",
    "#             row.extend([''] * (max_columns - len(row)))\n",
    "    \n",
    "#     # Write the header and context rows to a new CSV file.\n",
    "#     with open(output_file, 'w', newline='') as f_out:\n",
    "#         writer = csv.writer(f_out)\n",
    "#         writer.writerow(header)\n",
    "#         writer.writerows(context_data)\n",
    "    \n",
    "#     print(f\"Exported header and lines {start_line} to {end_line} to {output_file}.\")\n",
    "\n",
    "# # For example, to export context around the offending line 5109122:\n",
    "# export_context('csv_exports/bulk_results.csv', offending_line=5109122, context=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde085db-b638-412e-b649-c741a972421e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
